{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedaaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trainer import Trainer\n",
    "import re\n",
    "from collections import Counter\n",
    "from model import MultiHeadAttention, FeedForward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596bcc8",
   "metadata": {},
   "source": [
    "### Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"max_len\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 25,\n",
    "    \"lr\": 1e-3,\n",
    "    \"vocab_size\": 5000,\n",
    "    \"d_model\": 128,       # Embedding dimension\n",
    "    \"n_heads\": 4,         # Number of attention heads\n",
    "    \"n_layers\": 2,        # Number of Transformer blocks\n",
    "    \"dropout\": 0.1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe2538",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b324f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/skincancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635f56c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Patient History: Age: 8, Lesion region: arm, Lesion grew: false, Lesion itch: false, Lesion bled: false, Lesion hurt: false, Lesion changed: false, Lesion elevation: false.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf83005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnostic\n",
       "BCC    845\n",
       "ACK    730\n",
       "NEV    244\n",
       "SEK    235\n",
       "SCC    192\n",
       "MEL     52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnostic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping from diagnostic strings to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109d2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(df['diagnostic'].unique())\n",
    "label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "df['label'] = df['diagnostic'].map(label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77327b",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6aa095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1838\n",
      "Validation set size: 460\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['sentence'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbee88d",
   "metadata": {},
   "source": [
    "### Calculate Class Weights for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a85f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:\n",
      "  - Class 'ACK': 3.15\n",
      "  - Class 'BCC': 2.72\n",
      "  - Class 'MEL': 43.76\n",
      "  - Class 'NEV': 9.43\n",
      "  - Class 'SCC': 12.01\n",
      "  - Class 'SEK': 9.78\n"
     ]
    }
   ],
   "source": [
    "weights = 1 / (torch.bincount(torch.tensor(y_train.values)) / len(y_train.values)).to(CONFIG['device'])\n",
    "\n",
    "print(f\"Class Weights:\")\n",
    "for i, weight in enumerate(weights):\n",
    "    print(f\"  - Class '{int_to_label[i]}': {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc1899",
   "metadata": {},
   "source": [
    "### Simple Word-Level Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e55e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 189\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        def clean_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "            return text\n",
    "\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(clean_text(str(sentence)).split())\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        most_common_words = word_counts.most_common(self.vocab_size - 4)\n",
    "\n",
    "        self.word_to_idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n",
    "        for i, (word, _) in enumerate(most_common_words, 4):\n",
    "            self.word_to_idx[word] = i\n",
    "\n",
    "        self.idx_to_word = {i: w for w, i in self.word_to_idx.items()}\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        cleaned_sentence = str(sentence).lower()\n",
    "        cleaned_sentence = re.sub(r'[^a-z0-9\\s]', '', cleaned_sentence)\n",
    "        return cleaned_sentence.split()\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_to_idx.get(token, self.word_to_idx['[UNK]']) for token in tokens]\n",
    "    \n",
    "tokenizer = SimpleTokenizer(vocab_size=CONFIG['vocab_size'])\n",
    "tokenizer.build_vocab(X_train)\n",
    "print(f\"\\nVocabulary size: {len(tokenizer.word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5762555",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids = [self.tokenizer.word_to_idx['[CLS]']] + token_ids\n",
    "        token_ids = token_ids[:self.max_len]\n",
    "\n",
    "        padding_len = self.max_len - len(token_ids)\n",
    "        token_ids = token_ids + [self.tokenizer.word_to_idx['[PAD]']] * padding_len\n",
    "        attention_mask = [1 if id != self.tokenizer.word_to_idx['[PAD]'] else 0 for id in token_ids]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "train_dataset = SkinLesionDataset(X_train, y_train, tokenizer, CONFIG['max_len'])\n",
    "val_dataset = SkinLesionDataset(X_val, y_val, tokenizer, CONFIG['max_len'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1eb4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, head_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.fc_q = nn.Linear(d_model, head_dim)\n",
    "        self.fc_k = nn.Linear(d_model, head_dim)\n",
    "        self.fc_v = nn.Linear(d_model, head_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([head_dim])).to(CONFIG['device'])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q, K, V = self.fc_q(x), self.fc_k(x), self.fc_v(x)\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask.unsqueeze(1) == 0, -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        return torch.matmul(self.dropout(attention), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dfb9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            SingleHeadAttention(d_model, self.head_dim, dropout)\n",
    "            for _ in range(n_heads)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        head_outputs = [head(x, mask) for head in self.heads]\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc_out(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56bcc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, ff_dim)\n",
    "        self.linear_2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(self.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49acfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf99847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, num_classes, max_len, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, num_classes)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(CONFIG['device'])\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size, seq_len = src.shape\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(CONFIG['device'])\n",
    "        tok_emb = self.token_embedding(src) * self.scale\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        return self.fc_out(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b0372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 01 | Train Loss: 1.602 | Val. Loss: 1.461 | Val. Recall: 34.04%\n",
      "Epoch: 02 | Train Loss: 1.464 | Val. Loss: 1.502 | Val. Recall: 32.38%\n",
      "Epoch: 03 | Train Loss: 1.429 | Val. Loss: 1.395 | Val. Recall: 39.80%\n",
      "Epoch: 04 | Train Loss: 1.371 | Val. Loss: 1.413 | Val. Recall: 31.27%\n",
      "Epoch: 05 | Train Loss: 1.373 | Val. Loss: 1.328 | Val. Recall: 45.89%\n",
      "Epoch: 06 | Train Loss: 1.346 | Val. Loss: 1.339 | Val. Recall: 40.24%\n",
      "Epoch: 07 | Train Loss: 1.343 | Val. Loss: 1.418 | Val. Recall: 37.78%\n",
      "Epoch: 08 | Train Loss: 1.365 | Val. Loss: 1.362 | Val. Recall: 42.98%\n",
      "Epoch: 09 | Train Loss: 1.307 | Val. Loss: 1.296 | Val. Recall: 46.20%\n",
      "Epoch: 10 | Train Loss: 1.300 | Val. Loss: 1.507 | Val. Recall: 30.06%\n",
      "Epoch: 11 | Train Loss: 1.358 | Val. Loss: 1.407 | Val. Recall: 35.84%\n",
      "Epoch: 12 | Train Loss: 1.336 | Val. Loss: 1.314 | Val. Recall: 45.23%\n",
      "Epoch: 13 | Train Loss: 1.320 | Val. Loss: 1.260 | Val. Recall: 41.93%\n",
      "Epoch: 14 | Train Loss: 1.315 | Val. Loss: 1.266 | Val. Recall: 44.94%\n",
      "Epoch: 15 | Train Loss: 1.260 | Val. Loss: 1.371 | Val. Recall: 49.26%\n",
      "Epoch: 16 | Train Loss: 1.288 | Val. Loss: 1.269 | Val. Recall: 51.21%\n",
      "Epoch: 17 | Train Loss: 1.228 | Val. Loss: 1.296 | Val. Recall: 51.17%\n",
      "Epoch: 18 | Train Loss: 1.254 | Val. Loss: 1.264 | Val. Recall: 48.94%\n",
      "Epoch: 19 | Train Loss: 1.244 | Val. Loss: 1.240 | Val. Recall: 50.90%\n",
      "Epoch: 20 | Train Loss: 1.190 | Val. Loss: 1.175 | Val. Recall: 52.50%\n",
      "Epoch: 21 | Train Loss: 1.212 | Val. Loss: 1.247 | Val. Recall: 47.22%\n",
      "Epoch: 22 | Train Loss: 1.271 | Val. Loss: 1.261 | Val. Recall: 43.04%\n",
      "Epoch: 23 | Train Loss: 1.187 | Val. Loss: 1.163 | Val. Recall: 49.78%\n",
      "Epoch: 24 | Train Loss: 1.171 | Val. Loss: 1.255 | Val. Recall: 44.79%\n",
      "Epoch: 25 | Train Loss: 1.165 | Val. Loss: 1.445 | Val. Recall: 38.96%\n",
      "\n",
      "Final Evaluation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.91      0.21      0.34       146\n",
      "         BCC       0.62      0.59      0.61       169\n",
      "         MEL       0.00      0.00      0.00        10\n",
      "         NEV       0.46      0.65      0.54        49\n",
      "         SCC       0.17      0.46      0.25        39\n",
      "         SEK       0.35      0.43      0.38        47\n",
      "\n",
      "    accuracy                           0.43       460\n",
      "   macro avg       0.42      0.39      0.35       460\n",
      "weighted avg       0.62      0.43      0.45       460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MiniBERT(\n",
    "    vocab_size=len(tokenizer.word_to_idx),\n",
    "    d_model=CONFIG['d_model'],\n",
    "    n_layers=CONFIG['n_layers'],\n",
    "    n_heads=CONFIG['n_heads'],\n",
    "    num_classes=NUM_CLASSES,\n",
    "    max_len=CONFIG['max_len'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "trainer = Trainer(device=CONFIG['device'])\n",
    "trainer.fit(model,\n",
    "            CONFIG['lr'],\n",
    "            CONFIG['epochs'],\n",
    "            weights,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            NUM_CLASSES,\n",
    "            int_to_label\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
