{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedaaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from tokenizer import MostFrequentWordsTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import MultiHeadAttention, FeedForward\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import get_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596bcc8",
   "metadata": {},
   "source": [
    "### Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 512\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "MAX_EPOCHS = 75\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 256\n",
    "LEARNING_RATE = 5e-5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe2538",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b324f4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Patient History: Age: 8, Lesion region: arm, Lesion grew: false, Lesion itch: false, Lesion bled: false, Lesion hurt: false, Lesion changed: false, Lesion elevation: false.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/skincancer.csv')\n",
    "df['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf83005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnostic\n",
       "BCC    845\n",
       "ACK    730\n",
       "NEV    244\n",
       "SEK    235\n",
       "SCC    192\n",
       "MEL     52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnostic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c29f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'diagnostic'] = \\\n",
    "  df['diagnostic'].map({\n",
    "    'BCC': 'malignant',\n",
    "    'SCC': 'malignant',\n",
    "    'ACK': 'benign',\n",
    "    'NEV': 'benign',\n",
    "    'SEK': 'benign',\n",
    "    'MEL': 'malignant'\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping from diagnostic strings to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109d2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(df['diagnostic'].unique())\n",
    "label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "df['label'] = df['diagnostic'].map(label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77327b",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6aa095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1838\n",
      "Validation set size: 460\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['sentence'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc1899",
   "metadata": {},
   "source": [
    "### Simple Word-Level Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e55e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 190\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MostFrequentWordsTokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.build_vocab(X_train)\n",
    "print(f\"\\nVocabulary size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5762555",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids = self.tokenizer.encode(['[CLS]']) + token_ids\n",
    "        token_ids = token_ids[:self.max_len]\n",
    "\n",
    "        id_pad_token = self.tokenizer.encode(['[PAD]'])\n",
    "        padding_len = self.max_len - len(token_ids)\n",
    "        token_ids = token_ids + id_pad_token * padding_len\n",
    "        attention_mask = [1 if id != id_pad_token else 0 for id in token_ids]\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            'x': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "train_dataset = SkinLesionDataset(X_train, y_train, tokenizer, BLOCK_SIZE)\n",
    "val_dataset = SkinLesionDataset(X_val, y_val, tokenizer, BLOCK_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49acfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf99847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, num_classes, max_len, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, num_classes)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(DEVICE)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size, seq_len = src.shape\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(DEVICE)\n",
    "        tok_emb = self.token_embedding(src) * self.scale\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        return self.fc_out(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd82a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5261096605744126\n"
     ]
    }
   ],
   "source": [
    "weights = get_class_weights(NUM_CLASSES, df, y_train.values, int_to_label, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01b0372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 01 | Train Loss: 0.593 | Val. Loss: 0.461 | Val. Recall: 96.79% | Val. Precision: 69.64% | Val. FPR: 38.02%\n",
      "Epoch: 02 | Train Loss: 0.404 | Val. Loss: 0.418 | Val. Recall: 96.79% | Val. Precision: 69.64% | Val. FPR: 38.02%\n",
      "Epoch: 03 | Train Loss: 0.384 | Val. Loss: 0.462 | Val. Recall: 100.00% | Val. Precision: 68.99% | Val. FPR: 40.50%\n",
      "Epoch: 04 | Train Loss: 0.361 | Val. Loss: 0.433 | Val. Recall: 100.00% | Val. Precision: 68.99% | Val. FPR: 40.50%\n",
      "Epoch: 05 | Train Loss: 0.359 | Val. Loss: 0.414 | Val. Recall: 99.54% | Val. Precision: 68.89% | Val. FPR: 40.50%\n",
      "Epoch: 06 | Train Loss: 0.343 | Val. Loss: 0.433 | Val. Recall: 100.00% | Val. Precision: 68.99% | Val. FPR: 40.50%\n",
      "Epoch: 07 | Train Loss: 0.335 | Val. Loss: 0.398 | Val. Recall: 97.71% | Val. Precision: 69.61% | Val. FPR: 38.43%\n",
      "Epoch: 08 | Train Loss: 0.333 | Val. Loss: 0.419 | Val. Recall: 97.71% | Val. Precision: 69.38% | Val. FPR: 38.84%\n",
      "Epoch: 09 | Train Loss: 0.336 | Val. Loss: 0.448 | Val. Recall: 100.00% | Val. Precision: 68.99% | Val. FPR: 40.50%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MiniBERT(\n\u001b[1;32m      2\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocab_size(),\n\u001b[1;32m      3\u001b[0m     d_model\u001b[38;5;241m=\u001b[39mD_MODEL,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mDROPOUT\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(device\u001b[38;5;241m=\u001b[39mDEVICE, save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_bert.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES, weights\u001b[38;5;241m=\u001b[39mweights)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mint_to_label\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/life/workshop-sis/workshop-transformers/exemplos/trainer.py:22\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, learning_rate, max_epochs, weights, train_loader, val_loader, num_classes, int_to_label, threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> 22\u001b[0m \ttrain_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \tval_loss, val_recall, val_precision, val_fpr, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(model, val_loader, threshold)\n\u001b[1;32m     24\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_loss:\n",
      "File \u001b[0;32m~/life/workshop-sis/workshop-transformers/exemplos/trainer.py:55\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     53\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multiclass \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multiclass \u001b[38;5;28;01melse\u001b[39;00m label)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/gandalf/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gandalf/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gandalf/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MiniBERT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    num_classes=NUM_CLASSES if NUM_CLASSES > 2 else 1,\n",
    "    max_len=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "trainer = Trainer(device=DEVICE, save_name='mini_bert.pth', num_classes=NUM_CLASSES, weights=weights)\n",
    "trainer.fit(model,\n",
    "            LEARNING_RATE,\n",
    "            MAX_EPOCHS,\n",
    "            weights,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            NUM_CLASSES,\n",
    "            int_to_label\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
