{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f73bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tokenizer import MostFrequentWordsTokenizer, BytePairEncodingTokenizer\n",
    "from model import FeedForward, MultiHeadAttention\n",
    "from trainer import TrainerGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62f7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2000\n",
    "D_MODEL = 384\n",
    "N_HEADS = 6\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "MAX_ITERS = 2000\n",
    "EVAL_INTERVAL = 20\n",
    "BATCH_SIZE = 128\n",
    "BLOCK_SIZE = 256\n",
    "LEARNING_RATE = 3e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7afdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty\n",
       " Surgery                          1103\n",
       " Consult - History and Phy.        516\n",
       " Cardiovascular / Pulmonary        372\n",
       " Orthopedic                        355\n",
       " Radiology                         273\n",
       " General Medicine                  259\n",
       " Gastroenterology                  230\n",
       " Neurology                         223\n",
       " SOAP / Chart / Progress Notes     166\n",
       " Obstetrics / Gynecology           160\n",
       " Urology                           158\n",
       " Discharge Summary                 108\n",
       " ENT - Otolaryngology               98\n",
       " Neurosurgery                       94\n",
       " Hematology - Oncology              90\n",
       " Ophthalmology                      83\n",
       " Nephrology                         81\n",
       " Emergency Room Reports             75\n",
       " Pediatrics - Neonatal              70\n",
       " Pain Management                    62\n",
       " Psychiatry / Psychology            53\n",
       " Office Notes                       51\n",
       " Podiatry                           47\n",
       " Dermatology                        29\n",
       " Cosmetic / Plastic Surgery         27\n",
       " Dentistry                          27\n",
       " Letters                            23\n",
       " Physical Medicine - Rehab          21\n",
       " Sleep Medicine                     20\n",
       " Endocrinology                      19\n",
       " Bariatrics                         18\n",
       " IME-QME-Work Comp etc.             16\n",
       " Chiropractic                       14\n",
       " Rheumatology                       10\n",
       " Diets and Nutritions               10\n",
       " Speech - Language                   9\n",
       " Autopsy                             8\n",
       " Lab Medicine - Pathology            8\n",
       " Allergy / Immunology                7\n",
       " Hospice - Palliative Care           6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/anamnese.csv')\n",
    "text_corpus = df['transcription'].dropna().astype(str).tolist()\n",
    "text = \" \".join(text_corpus)\n",
    "df['medical_specialty'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987b909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subjective', 'this', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'she', 'used', 'to', 'have', 'allergies', 'when', 'she', 'lived', 'in', 'seattle', 'but', 'she', 'thinks', 'they', 'are', 'worse', 'here', 'in', 'the', 'past', 'she', 'has', 'tried', 'claritin', 'and', 'zyrtec', 'both', 'worked', 'for', 'short', 'time', 'but', 'then', 'seemed', 'to', 'lose', 'effectiveness', 'she', 'has', 'used', 'allegra', 'also', 'she', 'used', 'that', 'last', 'summer', 'and', 'she', 'began', 'using', 'it', 'again', 'two', 'weeks', 'ago', 'it', 'does', 'not', 'appear', 'to', 'be', 'working', 'very', 'well', 'she', 'has', 'used', 'overthecounter', 'sprays', 'but', 'no', 'prescription', 'nasal', 'sprays', 'she', 'does', 'have', 'asthma', 'but', 'doest', 'not', 'require', 'daily', 'medication', 'for', 'this', 'and', 'does', 'not', 'think', 'it', 'is', 'flaring', 'upmedications', 'her', 'only', 'medication', 'currently', 'is', 'ortho', 'tricyclen', 'and', 'the', 'allegraallergies', 'she', 'has', 'no', 'known', 'medicine', 'allergiesobjectivevitals', 'weight', 'was', '130', 'pounds', 'and', 'blood', 'pressure', '12478heent', 'her', 'throat', 'was', 'mildly', 'erythematous', 'without', 'exudate', 'nasal', 'mucosa', 'was', 'erythematous', 'and', 'swollen', 'only', 'clear', 'drainage', 'was', 'seen', 'tms', 'were', 'clearneck', 'supple', 'without', 'adenopathylungs', 'clearassessment', 'allergic', 'rhinitisplan1', 'she', 'will', 'try', 'zyrtec', 'instead', 'of', 'allegra', 'again', 'another', 'option', 'will', 'be', 'to', 'use', 'loratadine', 'she', 'does', 'not', 'think', 'she', 'has', 'prescription', 'coverage', 'so', 'that', 'might', 'be', 'cheaper2', 'samples', 'of', 'nasonex', 'two', 'sprays', 'in', 'each', 'nostril', 'given', 'for', 'three', 'weeks', 'a', 'prescription', 'was', 'written', 'as', 'well']\n",
      "SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MostFrequentWordsTokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.build_vocab(text_corpus)\n",
    "print(tokenizer.tokenize(text_corpus[0]))\n",
    "print(text_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f233857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['SU', 'B', 'J', 'EC', 'TI', 'VE', ':,', 'This', '2', '3', '-', 'year', '-', 'old', 'white', 'female', 'present', 's', 'with', 'complain', 't', 'of', 'allerg', 'ies', '.', 'She', 'used', 'to', 'have', 'allerg', 'ies', 'when', 'she', 'li', 'ved', 'in', 'Se', 'att', 'le', 'but', 'she', 'th', 'in', 'ks', 'they', 'are', 'wor', 'se', 'here', '.', 'In', 'the', 'past', ',', 'she', 'has', 'tr', 'ied', 'C', 'lar', 'it', 'in', ',', 'and', 'Z', 'y', 'r', 'te', 'c', '.', 'Bo', 'th', 'work', 'ed', 'for', 'short', 'time', 'but', 'then', 'se', 'em', 'ed', 'to', 'lo', 'se', 'ef', 'fect', 'iv', 'en', 'ess', '.', 'She', 'has', 'used', 'Al', 'le', 'gr', 'a', 'also', '.', 'She', 'used', 'that', 'last', 'su', 'mm', 'er', 'and', 'she', 'be', 'gan', 'using', 'it', 'again', 'two', 'weeks', 'ago', '.', 'It', 'does', 'not', 'appear', 'to', 'be', 'work', 'ing', 'very', 'well', '.', 'She', 'has', 'used', 'over', '-', 'the', '-', 'coun', 'ter', 'sp', 'ray', 's', 'but', 'no', 'pres', 'cr', 'ip', 'tion', 'nasal', 'sp', 'ray', 's', '.', 'She', 'does', 'have', 'as', 'th', 'ma', 'but', 'do', 'est', 'not', 'requ', 'ire', 'daily', 'med', 'ication', 'for', 'this', 'and', 'does', 'not', 'th', 'ink', 'it', 'is', 'f', 'lar', 'ing', 'up', '.,', 'MEDICATIONS', ':', ',', 'Her', 'only', 'med', 'ication', 'currently', 'is', 'O', 'r', 'th', 'o', 'T', 'r', 'i', '-', 'C', 'y', 'cl', 'en', 'and', 'the', 'Al', 'le', 'gr', 'a', '.,', 'ALLERGIES', ':', ',', 'She', 'has', 'no', 'known', 'med', 'ic', 'ine', 'allerg', 'ies', '.,', 'O', 'B', 'J', 'EC', 'TI', 'VE', ':,', 'V', 'ital', 's', ':', 'We', 'ight', 'was', '1', '30', 'pounds', 'and', 'blood', 'pressure', '12', '4', '/', '7', '8', '.,', 'HEENT', ':', 'Her', 'th', 'ro', 'at', 'was', 'mild', 'ly', 'ery', 'the', 'mat', 'ous', 'without', 'ex', 'ud', 'ate', '.', 'N', 'as', 'al', 'mucos', 'a', 'was', 'ery', 'the', 'mat', 'ous', 'and', 'sw', 'ol', 'len', '.', 'On', 'ly', 'clear', 'drain', 'age', 'was', 'seen', '.', 'T', 'M', 's', 'were', 'clear', '.,', 'Ne', 'ck', ':', 'S', 'up', 'ple', 'without', 'aden', 'opathy', '.,', 'L', 'ung', 's', ':', 'Clear', '.,', 'ASSESSMENT', ':,', 'All', 'erg', 'ic', 'rh', 'in', 'itis', '.,', 'PLAN', ':,', '1', '.', 'She', 'will', 'try', 'Z', 'y', 'r', 'te', 'c', 'in', 'ste', 'ad', 'of', 'Al', 'le', 'gr', 'a', 'again', '.', 'A', 'no', 'ther', 'op', 'tion', 'will', 'be', 'to', 'use', 'l', 'or', 'at', 'ad', 'ine', '.', 'She', 'does', 'not', 'th', 'ink', 'she', 'has', 'pres', 'cr', 'ip', 'tion', 'co', 'ver', 'age', 'so', 'that', 'm', 'ight', 'be', 'che', 'ap', 'er', '.,', '2', '.', 'S', 'am', 'ple', 's', 'of', 'N', 'as', 'on', 'ex', 'two', 'sp', 'ray', 's', 'in', 'each', 'no', 'str', 'il', 'given', 'for', 'three', 'weeks', '.', 'A', 'pres', 'cr', 'ip', 'tion', 'was', 'w', 'r', 'it', 'ten', 'as', 'well', '.']\n",
      "SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BytePairEncodingTokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.build_vocab(text_corpus)\n",
    "print(tokenizer.tokenize(text_corpus[0]))\n",
    "print(text_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89550642",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data \n",
    "  ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "  x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "  return x.to(DEVICE), y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75918810",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "random_ expects 'from' to be less than 'to', but got from=0 >= to=-256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m train_data \u001b[38;5;241m=\u001b[39m get_dataset(data, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m val_data \u001b[38;5;241m=\u001b[39m get_dataset(data, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     22\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()), tokenizer\u001b[38;5;241m.\u001b[39mdecode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mget_dataset.<locals>.get_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m():\n\u001b[0;32m----> 9\u001b[0m   ix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mBLOCK_SIZE] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m     11\u001b[0m   y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mBLOCK_SIZE\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: random_ expects 'from' to be less than 'to', but got from=0 >= to=-256"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "def get_dataset(data, train_size=0.9, split='train'):\n",
    "  n = int(train_size * len(data))\n",
    "  train_data, val_data = data[:n], data[n:]\n",
    "  data = train_data if split == 'train' else val_data\n",
    "\n",
    "  def get_batch():\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "  return get_batch\n",
    "\n",
    "train_data = get_dataset(data, split='train')\n",
    "val_data = get_dataset(data, split='val')\n",
    "\n",
    "x, y = train_data()\n",
    "x.shape, y.shape\n",
    "\n",
    "tokenizer.decode(x[0].tolist()), tokenizer.decode(y[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout, ff_dim):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.feed_forward_norm = nn.LayerNorm(d_model)\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # --- Self-attention sublayer ---\n",
    "        attention_output = self.self_attention(x, mask)\n",
    "        x = self.attention_norm(x + self.residual_dropout(attention_output))\n",
    "\n",
    "        # --- Feed-forward sublayer ---\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        x = self.feed_forward_norm(x + self.residual_dropout(feed_forward_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        num_layers,\n",
    "        block_size,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # --- Embedding tables ---\n",
    "        self.output_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        # --- Decoder stack ---\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderBlock(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    dropout=dropout,\n",
    "                    ff_dim=d_model * 4,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(block_size, block_size)).unsqueeze(0)\n",
    "        self.register_buffer(\"causal_mask\", causal_mask)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        batch_size, sequence_length = token_ids.shape\n",
    "\n",
    "        token_vectors = self.output_embedding(token_ids)\n",
    "        position_indices = torch.arange(sequence_length, device=token_ids.device)\n",
    "        position_vectors = self.position_encoding(position_indices)\n",
    "\n",
    "        x = token_vectors + position_vectors\n",
    "\n",
    "        causal_mask = self.causal_mask[:, :sequence_length, :sequence_length]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask)\n",
    "\n",
    "        # --- Project back to token logits ---\n",
    "        x = self.final_norm(x)\n",
    "        return self.output_projection(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits_last_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_last_step, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniGPT(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS, BLOCK_SIZE).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c49680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, targets):\n",
    "    return nn.functional.cross_entropy(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_validation_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_INTERVAL)\n",
    "    for k in range(EVAL_INTERVAL):\n",
    "        X, Y = get_batch('val')\n",
    "        losses[k] = get_loss(X, Y).item()\n",
    "    loss = losses.mean()\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5453df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting MiniGPT training...\n",
      "Prompt: The patient has a history of \n",
      "\n",
      "--- Generated Text ---\n",
      "The patient has a history of the medial inter es les ation io De were con bral as PERFORMED; hand ital ro, fever oma foot see ac and art\n",
      "step 20: train loss 6.3953, val loss 5.9658, val perplexity 389.8454\n",
      "Prompt: The patient has a history of \n",
      "\n",
      "--- Generated Text ---\n",
      "The patient has a history of appear and second ro it ts, plan was discharged as ment and sub in x of the long free came in the se of\n",
      "step 40: train loss 5.4721, val loss 5.1372, val perplexity 170.2419\n",
      "Prompt: The patient has a history of \n",
      "\n",
      "--- Generated Text ---\n",
      "The patient has a history of En any evidence / 60 W headache to the patient is normal lim its findings. The g ain his vis it ory as ound\n",
      "step 60: train loss 4.7576, val loss 4.6132, val perplexity 100.8030\n",
      "Prompt: The patient has a history of \n",
      "\n",
      "--- Generated Text ---\n",
      "The patient has a history of cr hel m ye lo o em p ic ul a ur ine man ent and fat ly increased 7, lymph sutures., CON\n",
      "step 80: train loss 4.3980, val loss 4.3770, val perplexity 79.5949\n",
      "Prompt: The patient has a history of \n",
      "\n",
      "--- Generated Text ---\n",
      "The patient has a history of ef fort in P r hea. She is not to ge ther that she has one symptoms well ian is clear that she reported\n",
      "step 100: train loss 4.2240, val loss 4.2148, val perplexity 67.6789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TrainerGPT()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_ITERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe patient has a history of \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/life/workshop-sis/workshop-transformers/exemplos/trainer.py:133\u001b[0m, in \u001b[0;36mTrainerGPT.fit\u001b[0;34m(self, model, data, learning_rate, max_iters, eval_interval, tokenizer, device, block_size, batch_size, eval_prompt)\u001b[0m\n\u001b[1;32m    130\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val perplexity \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_perplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \ttrain_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 133\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(X\u001b[38;5;241m.\u001b[39mto(device), Y\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    136\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss(logits, Y\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/life/workshop-sis/workshop-transformers/exemplos/trainer.py:197\u001b[0m, in \u001b[0;36mcreate_get_batch_function.<locals>.get_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m    196\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = TrainerGPT()\n",
    "\n",
    "trainer.fit(model, data, max_iters=MAX_ITERS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    tokenizer=tokenizer,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    eval_prompt=\"The patient has a history of \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
