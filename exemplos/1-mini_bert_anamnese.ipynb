{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e739ad",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedaaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from pprint import pprint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from imblearn.metrics import specificity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cfd71",
   "metadata": {},
   "source": [
    "# Configuração e hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "D_MODEL = 384     #768\n",
    "N_HEADS = 6       #12\n",
    "N_LAYERS = 2      #12\n",
    "DROPOUT = 0.1\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25ded2",
   "metadata": {},
   "source": [
    "# 1. Classes Utilitárias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1332af",
   "metadata": {},
   "source": [
    "## 1.1 Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7737e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class MostFrequentWordsTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "\n",
    "    def clean_sentence(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        return re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(self.clean_sentence(str(sentence)).split())\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        most_common_words = word_counts.most_common(self.vocab_size - 4)\n",
    "        \n",
    "        self.token_to_idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3} # Tokens especiais\n",
    "        for i, (word, _) in enumerate(most_common_words, 4):\n",
    "            self.token_to_idx[word] = i\n",
    "\n",
    "        self.idx_to_token = {i: w for w, i in self.token_to_idx.items()}\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        cleaned_sentence = self.clean_sentence(str(sentence))\n",
    "        return cleaned_sentence.split()\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_idx.get(token, self.token_to_idx['[UNK]']) for token in tokens]\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        tokens = ['[CLS]'] + self.tokenize(sentence) + ['[SEP]']\n",
    "        return self.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return ' '.join([self.idx_to_token.get(i, '[UNK]') for i in ids])\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5be910",
   "metadata": {},
   "source": [
    "## 1.2 Camadas do Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fec2bb",
   "metadata": {},
   "source": [
    "### 1.2.1 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2cc0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, head_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.fc_q = nn.Linear(d_model, head_dim)\n",
    "        self.fc_k = nn.Linear(d_model, head_dim)\n",
    "        self.fc_v = nn.Linear(d_model, head_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('scale', torch.sqrt(torch.FloatTensor([head_dim])))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q, K, V = self.fc_q(x), self.fc_k(x), self.fc_v(x)\n",
    "        qV = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            qV = qV.masked_fill(mask == 0, -1e10)\n",
    "        attention = torch.softmax(qV, dim=-1)\n",
    "        return torch.matmul(self.dropout(attention), V) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5e7fa",
   "metadata": {},
   "source": [
    "### 1.2.2 Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99df735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SingleHeadAttention(d_model, d_model // n_heads, dropout)\n",
    "            for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        head_outputs = [head(x, mask) for head in self.heads]\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc_out(concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e927a1",
   "metadata": {},
   "source": [
    "### 1.2.3 Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d22477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe2538",
   "metadata": {},
   "source": [
    "# 2. Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddcf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/transcription.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad198460",
   "metadata": {},
   "source": [
    "## 2.1 Remove linhas sem transcrição ou especialidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fe7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['transcription', 'medical_specialty'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25bcdfe",
   "metadata": {},
   "source": [
    "## 2.1 Seleciona especialidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b324f4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty\n",
       " Cardiovascular / Pulmonary    371\n",
       " Orthopedic                    355\n",
       " Radiology                     273\n",
       " General Medicine              259\n",
       " Gastroenterology              224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['medical_specialty'].isin(\n",
    "  [' Cardiovascular / Pulmonary',\n",
    "   ' Orthopedic', ' Radiology', ' General Medicine', ' Gastroenterology', 'Neurology',\n",
    "  ])].reset_index(drop=True)\n",
    "\n",
    "df['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd171ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CC: ,Headache.,HX:, This 51 y/o RHM was moving furniture several days prior '\n",
      " 'to presentation when he struck his head (vertex) against a door panel. He '\n",
      " 'then stepped back and struck his back on a trailer hitch. There was no '\n",
      " 'associated LOC but he felt \"dazed.\" He complained a HA since the accident. '\n",
      " 'The following day he began experiencing episodic vertigo lasting several '\n",
      " 'minutes with associated nausea and vomiting. He has been lying in bed most '\n",
      " 'of the time since the accident.')\n",
      "Especialidade:  Radiology\n"
     ]
    }
   ],
   "source": [
    "pprint(df['transcription'][50][:475])\n",
    "print(f'Especialidade: {df[\"medical_specialty\"][50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69aaa5",
   "metadata": {},
   "source": [
    "# 3. Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Transformando especialidade em números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109d2b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    371\n",
       "3    355\n",
       "4    273\n",
       "2    259\n",
       "1    224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(df['medical_specialty'].unique())\n",
    "\n",
    "label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "\n",
    "df['label'] = df['medical_specialty'].map(label_to_int)\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98957d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes: 5\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(labels)\n",
    "print(f\"Número de classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77327b",
   "metadata": {},
   "source": [
    "## 3.2 Divindo conjunto de treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6aa095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 1185\n",
      "Validação: 297\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['transcription'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "print(f\"Treino: {len(X_train)}\")\n",
    "print(f\"Validação: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36797d85",
   "metadata": {},
   "source": [
    "## 3.3 Tokenizando o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e55e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamanho do vocabulário: 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['joe', 'broke', 'his', 'leg', 'playing', 'soccer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MostFrequentWordsTokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.build_vocab(X_train)\n",
    "print(f\"\\nTamanho do vocabulário: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "tokenizer.tokenize(\"Joe broke his leg playing soccer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98f1e0",
   "metadata": {},
   "source": [
    "## 3.4 Criando um dataset para carregar o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, block_size):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        # Tokeniza a frase\n",
    "        token_ids = self.tokenizer.encode(sentence)\n",
    "\n",
    "        # Trunca frases maiores que block_size\n",
    "        token_ids = token_ids[:self.block_size] \n",
    "\n",
    "        # Completa frases menores que block_size com [PAD]\n",
    "        id_pad_token = self.tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
    "        padding_len = self.block_size - len(token_ids) \n",
    "        token_ids = token_ids + id_pad_token * padding_len\n",
    "\n",
    "        # Cria máscara de atenção para ignorar tokens de padding\n",
    "        attention_mask = [1 if id != id_pad_token else 0 for id in token_ids]\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            'text': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = SkinLesionDataset(X_train, y_train, tokenizer, BLOCK_SIZE)\n",
    "val_dataset = SkinLesionDataset(X_val, y_val, tokenizer, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e2d6e",
   "metadata": {},
   "source": [
    "# 4. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab84e2",
   "metadata": {},
   "source": [
    "## 4.1 Implementando Encoder do Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49acfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask=None):\n",
    "        attn_output = self.attention(embeddings, mask)\n",
    "        x = self.norm1(embeddings + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748b5eb",
   "metadata": {},
   "source": [
    "## 4.2 Implementando BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf99847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, num_classes, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.linear = nn.Linear(d_model, num_classes)\n",
    "        self.register_buffer('scale', torch.sqrt(torch.FloatTensor([d_model])))\n",
    "        self.register_buffer('pos', torch.arange(block_size).unsqueeze(0))\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        input_emb = self.input_embedding(src) * self.scale\n",
    "        positional_emb = self.position_embedding(self.pos)\n",
    "        embeddings = self.embed_layer_norm(input_emb + positional_emb)\n",
    "        x = self.dropout(embeddings)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        return self.linear(cls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582cdf9",
   "metadata": {},
   "source": [
    "# 5. Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "961191cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\tdef __init__(self, device, save_name, weights):\n",
    "\t\tself.device = device\n",
    "\t\tself.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\t\tself.save_name = save_name\n",
    "\n",
    "\tdef fit(self, model, learning_rate, max_epochs, train_loader, val_loader):\n",
    "\t\t# configura o otimizador\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\t\tprint(\"\\nIniciando Treinamento...\")\n",
    "\n",
    "\t\tself.min_loss = float('inf')\n",
    "\t\tfor epoch in range(max_epochs):\n",
    "\t\t\t# treina uma época\n",
    "\t\t\ttrain_loss = self._train_epoch(model, train_loader, optimizer, self.criterion)\n",
    "\n",
    "\t\t\t# avalia no conjunto de validação\n",
    "\t\t\teval_validation = self.evaluate(model, val_loader)\n",
    "\n",
    "\t\t\t# se a loss de validação for a menor já vista\n",
    "\t\t\tif eval_validation['loss'] < self.min_loss:\n",
    "\t\t\t\tself.min_loss = eval_validation['loss']\n",
    "\t\t\t\t# salva o modelo com menor loss de validação\n",
    "\t\t\t\ttorch.save(model.state_dict(), f'{self.save_name}')\n",
    "\n",
    "\t\t\tprint(f\"Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {eval_validation['loss']:.3f} | Val. Recall: {eval_validation['recall']*100:.2f}% | Val. FPR: {eval_validation['fpr']*100:.2f}%\")\n",
    "\n",
    "\t\t# Após treinar, carrega o melhor modelo salvo\n",
    "\t\tmodel.load_state_dict(torch.load(self.save_name))\n",
    "\n",
    "\tdef _train_epoch(self, model, iterator, optimizer, criterion):\n",
    "\t\tmodel.train()\n",
    "\t\tepoch_loss = 0\n",
    "\t\tfor batch in iterator:\n",
    "\t\t\toutput, label = self.get_logits_targets(model, batch)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = criterion(output, label)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tepoch_loss += loss.item()\n",
    "\t\treturn epoch_loss / len(iterator)\n",
    "\n",
    "\tdef evaluate(self, model, iterator):\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\tepoch_loss = 0\n",
    "\t\tall_preds, all_labels = [], []\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor batch in iterator:\n",
    "\t\t\t\toutput, label = self.get_logits_targets(model, batch)\n",
    "\t\t\t\tloss = self.criterion(output, label)\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "\t\t\t\tpreds = torch.argmax(output, dim=1)\n",
    "\t\t\t\tall_preds.extend(preds.cpu().numpy())\n",
    "\t\t\t\tall_labels.extend(label.cpu().numpy())\n",
    "\n",
    "\t\tall_preds = np.array(all_preds)\n",
    "\t\tall_labels = np.array(all_labels)\n",
    "\n",
    "\t\trecall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\t\tprecision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\t\tfpr = 1 - specificity_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'fpr': fpr,\n",
    "\t\t\t'loss': epoch_loss / len(iterator),\n",
    "\t\t\t'recall': recall,\n",
    "\t\t\t'precision': precision,\n",
    "\t\t\t'all_preds': all_preds,\n",
    "\t\t\t'all_labels': all_labels,\n",
    "\t\t}\n",
    "\n",
    "\tdef get_logits_targets(self, model, batch):\n",
    "\t\tx, label, mask = batch['text'].to(self.device), batch['label'].to(self.device), batch['mask'].to(self.device) \n",
    "\t\treturn model(x, mask), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3e830",
   "metadata": {},
   "source": [
    "## 5.1 Defindo pesos para cada especialidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfbd82a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso da classe:\n",
      "' Cardiovascular / Pulmonary': 3.99\n",
      "' Gastroenterology': 6.62\n",
      "' General Medicine': 5.72\n",
      "' Orthopedic': 4.17\n",
      "' Radiology': 5.44\n"
     ]
    }
   ],
   "source": [
    "def get_class_weights(y_train, int_to_label, device):\n",
    "  weights = 1 / (torch.bincount(torch.tensor(y_train)) / len(y_train)).to(device)\n",
    "  print(f\"Peso da classe:\")\n",
    "  for i, weight in enumerate(weights):\n",
    "      print(f\"'{int_to_label[i]}': {weight:.2f}\")\n",
    "\n",
    "weights = get_class_weights(y_train.values, int_to_label, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a92c4b",
   "metadata": {},
   "source": [
    "## 5.2 Otimizando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b0372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Treinamento...\n",
      "Epoch: 01 | Train Loss: 1.637 | Val. Loss: 1.523 | Val. Recall: 30.66% | Val. FPR: 17.37%\n",
      "Epoch: 02 | Train Loss: 1.521 | Val. Loss: 1.413 | Val. Recall: 38.56% | Val. FPR: 15.42%\n",
      "Epoch: 03 | Train Loss: 1.379 | Val. Loss: 1.308 | Val. Recall: 42.61% | Val. FPR: 14.09%\n",
      "Epoch: 04 | Train Loss: 1.251 | Val. Loss: 1.126 | Val. Recall: 50.96% | Val. FPR: 12.10%\n",
      "Epoch: 05 | Train Loss: 1.148 | Val. Loss: 1.066 | Val. Recall: 60.29% | Val. FPR: 10.00%\n",
      "Epoch: 06 | Train Loss: 1.066 | Val. Loss: 1.025 | Val. Recall: 57.15% | Val. FPR: 10.58%\n",
      "Epoch: 07 | Train Loss: 0.988 | Val. Loss: 0.955 | Val. Recall: 62.70% | Val. FPR: 9.29%\n",
      "Epoch: 08 | Train Loss: 0.908 | Val. Loss: 0.946 | Val. Recall: 64.66% | Val. FPR: 8.91%\n",
      "Epoch: 09 | Train Loss: 0.900 | Val. Loss: 1.057 | Val. Recall: 59.45% | Val. FPR: 10.11%\n",
      "Epoch: 10 | Train Loss: 0.837 | Val. Loss: 0.982 | Val. Recall: 61.37% | Val. FPR: 9.47%\n",
      "Epoch: 11 | Train Loss: 0.771 | Val. Loss: 0.938 | Val. Recall: 65.76% | Val. FPR: 8.53%\n",
      "Epoch: 12 | Train Loss: 0.710 | Val. Loss: 0.884 | Val. Recall: 64.04% | Val. FPR: 8.75%\n",
      "Epoch: 13 | Train Loss: 0.689 | Val. Loss: 0.881 | Val. Recall: 64.57% | Val. FPR: 8.76%\n",
      "Epoch: 14 | Train Loss: 0.617 | Val. Loss: 0.796 | Val. Recall: 69.58% | Val. FPR: 7.69%\n",
      "Epoch: 15 | Train Loss: 0.575 | Val. Loss: 0.785 | Val. Recall: 68.18% | Val. FPR: 7.85%\n",
      "Epoch: 16 | Train Loss: 0.553 | Val. Loss: 0.903 | Val. Recall: 65.61% | Val. FPR: 8.31%\n",
      "Epoch: 17 | Train Loss: 0.516 | Val. Loss: 0.881 | Val. Recall: 65.41% | Val. FPR: 8.51%\n",
      "Epoch: 18 | Train Loss: 0.523 | Val. Loss: 0.901 | Val. Recall: 69.00% | Val. FPR: 7.79%\n",
      "Epoch: 19 | Train Loss: 0.503 | Val. Loss: 0.849 | Val. Recall: 67.20% | Val. FPR: 8.18%\n",
      "Epoch: 20 | Train Loss: 0.459 | Val. Loss: 0.896 | Val. Recall: 66.60% | Val. FPR: 8.18%\n"
     ]
    }
   ],
   "source": [
    "model = BERT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "trainer = Trainer(device=DEVICE, save_name='mini_bert_anamnese.pth', weights=weights)\n",
    "trainer.fit(model,\n",
    "            LEARNING_RATE,\n",
    "            MAX_EPOCHS,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852d98e",
   "metadata": {},
   "source": [
    "# 6. Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e469e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] history the patient is a [UNK] who presented with respiratory distress '\n",
      " 'and absent femoral pulses with subsequent evaluation including '\n",
      " 'echocardiogram that demonstrated severe [UNK] of the aorta with a peak '\n",
      " 'gradient of 29 mmhg and associated dilated cardiomyopathy with [UNK] [UNK] '\n",
      " 'of 16 a [UNK] aortic valve was also seen without insufficiency or stenosis '\n",
      " 'the patient underwent cardiac catheterization for balloon angioplasty for '\n",
      " '[UNK] of the [UNK] after sedation and general endotracheal anesthesia the '\n",
      " 'patient was prepped and draped cardiac catheterization was performed as '\n",
      " 'outlined in the attached continuation sheets vascular entry was by '\n",
      " 'percutaneous technique and the patient was heparinized monitoring during the '\n",
      " 'procedure included continuous surface ecg continuous pulse oximetry and '\n",
      " 'cycled cuff blood pressures in addition to intravascular pressuresusing a '\n",
      " 'percutaneous technique a 4french 8 cm long double lumen central venous '\n",
      " 'catheter was inserted in the left femoral vein and sutured into place there '\n",
      " 'was good blood return from both the [UNK] a 4french sheath a 4french wedge '\n",
      " 'catheter was inserted into the right femoral vein and advanced through the '\n",
      " 'right heart structures out to the branch of pulmonary arteries the atrial '\n",
      " 'septum was not probe [UNK] a 4french sheath a 4french marker pigtail '\n",
      " 'catheter was inserted into the left femoral artery and advanced retrograde '\n",
      " 'to the descending aorta ascending aorta and left ventricle a descending '\n",
      " 'aortogram demonstrated discrete [UNK] of the aorta approximately 8 mm distal '\n",
      " 'to the origin of the left subclavian artery the transverse arch measured 5 '\n",
      " 'mm isthmus measured 47 mm and [UNK] measured 29 x 18 mm at the descending '\n",
      " 'aorta level the diaphragm measured 56 mm the pigtail catheter was exchanged '\n",
      " 'for a wedge catheter which was then directed into the right innominate '\n",
      " 'artery this catheter was exchanged over a wire for a [UNK] mini 6 x 2 cm '\n",
      " 'balloon catheter which was advanced across the [UNK] and inflated with '\n",
      " 'complete disappearance of discrete waist pressure pullback following '\n",
      " 'angioplasty however demonstrated a residual of [UNK] mmhg gradient repeat '\n",
      " 'angiogram showed mild improvement in degree of aortic narrowing the '\n",
      " 'angioplasty was then performed using a [UNK] mini 7 x 2 cm balloon catheter '\n",
      " 'with complete disappearance of mild waist the pigtail catheter was then '\n",
      " 'reintroduced for a pressure pullback measurement and final [UNK] were '\n",
      " 'calculated by the fick technique using an assumed oxygen [UNK] were obtained '\n",
      " 'with injection in the descending [UNK] angiography two normalappearing renal '\n",
      " 'collecting systems were visualized the catheters and sheaths were removed '\n",
      " 'and topical pressure applied for hemostasis the patient was returned to the '\n",
      " 'pediatric intensive care unit in satisfactory condition there were no '\n",
      " 'complicationsdiscussion oxygen consumption was assumed to be normal mixed '\n",
      " 'venous saturation was low due to mild systemic arterial desaturation and '\n",
      " 'anemia there is no evidence of significant intracardiac shunt further the '\n",
      " 'heart was [UNK] due to [UNK] [UNK] rightsided pressures were normal as was '\n",
      " 'the right pulmonary artery capillary wedge pressure with the awave similar '\n",
      " 'to the normal left ventricular enddiastolic pressure of 12 mmhg left '\n",
      " 'ventricular systolic pressure was mildly increased with a 60 mmhg systolic '\n",
      " 'gradient into the ascending aorta and a 29')\n",
      "Predição: [' Cardiovascular / Pulmonary']\n",
      "Verdadeiro: [' Cardiovascular / Pulmonary']\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "eval_validation = next(iter(val_loader))\n",
    "text, mask, label = eval_validation['text'].to(DEVICE), eval_validation['mask'].to(DEVICE), eval_validation['label'].to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pprint(tokenizer.decode(text[0, :].cpu().numpy().tolist()))\n",
    "    outputs = model(text, mask)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    print(f'Predição: {[int_to_label[p.item()] for p in predictions]}')\n",
    "    print(f'Verdadeiro: {[int_to_label[l.item()] for l in label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79fb67",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524c34f",
   "metadata": {},
   "source": [
    "1. O número máximo de tokens na sequência (BLOCK_SIZE) utilizado é adequado? Muitas sentenças estão sendo truncadas?\n",
    "\n",
    "2. Aumente o BLOCK_SIZE. Qual valor máximo suportado pela memória da GPU?\n",
    "\n",
    "3. Adapte o código acima para outra tarefa de classificação na sua área de pesquisa/atuação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
