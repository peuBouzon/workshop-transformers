{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eedaaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from tokenizer import MostFrequentWordsTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import MultiHeadAttention, FeedForward\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import get_class_weights\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596bcc8",
   "metadata": {},
   "source": [
    "## 0. Configuração e hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2048\n",
    "D_MODEL = 384\n",
    "N_HEADS = 6\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "MAX_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe2538",
   "metadata": {},
   "source": [
    "## 1 - Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b324f4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty\n",
       " Cardiovascular / Pulmonary    371\n",
       " Orthopedic                    355\n",
       " Radiology                     273\n",
       " General Medicine              259\n",
       " Gastroenterology              224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/transcription.csv')\n",
    "df.dropna(subset=['transcription', 'medical_specialty'], inplace=True)\n",
    "df = df[df['medical_specialty'].isin(\n",
    "  [' Cardiovascular / Pulmonary',\n",
    "   ' Orthopedic', ' Radiology', ' General Medicine', ' Gastroenterology', 'Neurology',\n",
    "  ])].reset_index(drop=True)\n",
    "df['medical_specialty'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd171ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CC: ,Headache.,HX:, This 51 y/o RHM was moving furniture several days prior '\n",
      " 'to presentation when he struck his head (vertex) against a door panel. He '\n",
      " 'then stepped back and struck his back on a trailer hitch. There was no '\n",
      " 'associated LOC but he felt \"dazed.\" He complained a HA since the accident. '\n",
      " 'The following day he began experiencing episodic vertigo lasting several '\n",
      " 'minutes with associated nausea and vomiting. He has been lying in bed most '\n",
      " 'of the time since the accident.')\n",
      "Especialidade:  Radiology\n"
     ]
    }
   ],
   "source": [
    "pprint(df['transcription'][50][:475])\n",
    "print(f'Especialidade: {df[\"medical_specialty\"][50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69aaa5",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transformando especialidade em números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "109d2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(df['medical_specialty'].unique())\n",
    "\n",
    "label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "\n",
    "df['label'] = df['medical_specialty'].map(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a98957d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes: 5\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(labels)\n",
    "print(f\"Número de classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77327b",
   "metadata": {},
   "source": [
    "### 2.2 Divindo conjunto de treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6aa095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 1185\n",
      "Validação: 297\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['transcription'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "print(f\"Treino: {len(X_train)}\")\n",
    "print(f\"Validação: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36797d85",
   "metadata": {},
   "source": [
    "### 2.3 Tokenizando o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e55e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamanho do vocabulário: 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['joe', 'broke', 'his', 'leg', 'playing', 'soccer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MostFrequentWordsTokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.build_vocab(X_train)\n",
    "print(f\"\\nTamanho do vocabulário: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "tokenizer.tokenize(\"Joe broke his leg playing soccer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98f1e0",
   "metadata": {},
   "source": [
    "### 2.4 Criando um dataset para carregar o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, block_size):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        # Tokeniza a frase\n",
    "        token_ids = self.tokenizer.encode(sentence)\n",
    "\n",
    "        # Trunca frases maiores que block_size\n",
    "        token_ids = token_ids[:self.block_size] \n",
    "\n",
    "        # Completa frases menores que max_len com [PAD]\n",
    "        id_pad_token = self.tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
    "        padding_len = self.block_size - len(token_ids) \n",
    "        token_ids = token_ids + id_pad_token * padding_len\n",
    "\n",
    "        # Cria máscara de atenção para ignorar tokens de padding\n",
    "        attention_mask = [1 if id != id_pad_token else 0 for id in token_ids]\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            'text': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = SkinLesionDataset(X_train, y_train, tokenizer, BLOCK_SIZE)\n",
    "val_dataset = SkinLesionDataset(X_val, y_val, tokenizer, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e2d6e",
   "metadata": {},
   "source": [
    "## 3. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab84e2",
   "metadata": {},
   "source": [
    "### 3.1 Implementando Encoder do Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49acfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748b5eb",
   "metadata": {},
   "source": [
    "### 3.2 Implementando BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf99847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, num_classes, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.linear = nn.Linear(d_model, num_classes)\n",
    "        self.register_buffer('scale', torch.sqrt(torch.FloatTensor([d_model])))\n",
    "        self.register_buffer('pos', torch.arange(block_size).unsqueeze(0))\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        input_emb = self.input_embedding(src) * self.scale\n",
    "        positional_emb = self.position_embedding(self.pos)\n",
    "        x = self.dropout(self.embed_layer_norm(input_emb + positional_emb))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        return self.linear(cls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582cdf9",
   "metadata": {},
   "source": [
    "## 4. Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3e830",
   "metadata": {},
   "source": [
    "### 4.1 Defindo pesos para cada especialidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfbd82a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso da classe:\n",
      "' Cardiovascular / Pulmonary': 3.99\n",
      "' Gastroenterology': 6.62\n",
      "' General Medicine': 5.72\n",
      "' Orthopedic': 4.17\n",
      "' Radiology': 5.44\n"
     ]
    }
   ],
   "source": [
    "weights = get_class_weights(NUM_CLASSES, df, y_train.values, int_to_label, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a92c4b",
   "metadata": {},
   "source": [
    "### 4.2 Otimizando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01b0372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Treinamento...\n",
      "Epoch: 01 | Train Loss: 1.610 | Val. Loss: 1.589 | Val. Recall: 19.82% | Val. FPR: 20.06%\n",
      "Epoch: 02 | Train Loss: 1.483 | Val. Loss: 1.472 | Val. Recall: 36.99% | Val. FPR: 15.70%\n",
      "Epoch: 03 | Train Loss: 1.422 | Val. Loss: 1.311 | Val. Recall: 49.10% | Val. FPR: 12.75%\n",
      "Epoch: 04 | Train Loss: 1.282 | Val. Loss: 1.213 | Val. Recall: 52.14% | Val. FPR: 12.04%\n",
      "Epoch: 05 | Train Loss: 1.286 | Val. Loss: 1.161 | Val. Recall: 57.02% | Val. FPR: 10.99%\n",
      "Epoch: 06 | Train Loss: 1.092 | Val. Loss: 1.106 | Val. Recall: 56.47% | Val. FPR: 10.96%\n",
      "Epoch: 07 | Train Loss: 0.978 | Val. Loss: 1.043 | Val. Recall: 60.80% | Val. FPR: 9.91%\n",
      "Epoch: 08 | Train Loss: 0.963 | Val. Loss: 1.044 | Val. Recall: 58.94% | Val. FPR: 9.95%\n",
      "Epoch: 09 | Train Loss: 0.886 | Val. Loss: 0.946 | Val. Recall: 63.44% | Val. FPR: 9.03%\n",
      "Epoch: 10 | Train Loss: 0.816 | Val. Loss: 0.973 | Val. Recall: 63.12% | Val. FPR: 8.86%\n"
     ]
    }
   ],
   "source": [
    "model = BERT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "trainer = Trainer(device=DEVICE, save_name='mini_bert_anamnese.pth', num_classes=NUM_CLASSES, weights=weights)\n",
    "trainer.fit(model,\n",
    "            LEARNING_RATE,\n",
    "            MAX_EPOCHS,\n",
    "            weights,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            NUM_CLASSES,\n",
    "            int_to_label,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1e469e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] subjective review of the medical [UNK] shows that the patient is a '\n",
      " '[UNK] female patient who has been admitted and has been treated for [UNK] '\n",
      " '[UNK] pneumonia along with copd [UNK] the patient does have a [UNK] history '\n",
      " 'of copd however she does not use oxygen at her [UNK] [UNK] living home '\n",
      " 'yesterday she had made improvement since being here at the hospital she '\n",
      " 'needed oxygen she was [UNK] for home o2 and [UNK] for it yesterday also her '\n",
      " 'lungs were very [UNK] she did have [UNK] bilaterally and [UNK] on the right '\n",
      " 'side [UNK] she appeared to be a bit [UNK] and although she was [UNK] to be '\n",
      " 'discharged home she did not appear to be fit for [UNK] the patient needed to '\n",
      " 'use the rest room she stated that she needed to [UNK] she [UNK] decided not '\n",
      " 'to call for [UNK] she stated that she did have her [UNK] call light [UNK] '\n",
      " 'next to her and she was unable to [UNK] access to her [UNK] she attempted to '\n",
      " 'walk to the rest room on her [UNK] she sustained a fall she stated that she '\n",
      " 'just felt [UNK] she [UNK] her knee and her elbow she had femur xrays knee '\n",
      " 'xrays also there was possibility of [UNK] fracture and some swelling of her '\n",
      " '[UNK] [UNK] on the right side this morning she denied any headache back pain '\n",
      " 'or neck pain she complained [UNK] of right anterior knee pain for which she '\n",
      " 'had some [UNK] and [UNK] signs the patients [UNK] temperature over the past '\n",
      " '24 hours was [UNK] her blood pressure is [UNK] her pulse is [UNK] to [UNK] '\n",
      " 'she is 95 on 2 [UNK] via nasal [UNK] regular rate and rhythm without murmur '\n",
      " '[UNK] or [UNK] reveal no [UNK] wheezing throughout she does have some [UNK] '\n",
      " 'on the right mid base she did have a [UNK] cough this morning and she is '\n",
      " '[UNK] [UNK] purulent sputum [UNK] soft and nontender her bowel sounds [UNK] '\n",
      " 'are [UNK] she is alert and oriented x3 her pupils are equal and reactive she '\n",
      " 'has got a good head and facial muscle strength her tongue is midline she has '\n",
      " 'got clear speech her extraocular [UNK] are intact her spine is nontender on '\n",
      " 'palpation from neck to lumbar spine she has good range of motion with [UNK] '\n",
      " 'to her [UNK] [UNK] [UNK] and [UNK] her [UNK] [UNK] are equal bilaterally '\n",
      " 'both [UNK] are [UNK] from extension to flexion her hip [UNK] and [UNK] are '\n",
      " 'also [UNK] and equal bilaterally extension and flexion of the knee '\n",
      " 'bilaterally and [UNK] also are [UNK] palpation of her right knee reveals no '\n",
      " '[UNK] she does have [UNK] inflammation with some [UNK] and swelling she has '\n",
      " 'got good joint range of motion [UNK] she did have a skin tear involving her '\n",
      " 'right forearm lateral which is approximately 2 to 25 inches in length and is '\n",
      " 'at this time currently [UNK] and [UNK] with [UNK] and is not [UNK] [UNK] '\n",
      " 'acute on chronic copd [UNK] [UNK] [UNK] pneumonia both [UNK] however she may '\n",
      " 'need home o2 for a short period of')\n",
      "Predição: [' Orthopedic']\n",
      "Verdadeiro: [' General Medicine']\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "out = next(iter(val_loader))\n",
    "text, mask, label = out['text'].to(DEVICE), out['mask'].to(DEVICE), out['label'].to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pprint(tokenizer.decode(text[0, :].cpu().numpy().tolist()))\n",
    "    outputs = model(text, mask)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    print(f'Predição: {[int_to_label[p.item()] for p in predictions]}')\n",
    "    print(f'Verdadeiro: {[int_to_label[l.item()] for l in label]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
