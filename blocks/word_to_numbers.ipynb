{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f802e1e",
   "metadata": {},
   "source": [
    "## Preliminares: transformação de texto em números"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc268d",
   "metadata": {},
   "source": [
    "![Transformação de texto em números](tokenization.png \"Tranformação de texto em números\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053d369",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "8a487bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.'"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('mtsamples.csv')\n",
    "sentences = df['transcription'].dropna().astype(str).values\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674dc55",
   "metadata": {},
   "source": [
    "### Nível de caractere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "4627024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamanho do vocabulário: 84\n"
     ]
    }
   ],
   "source": [
    "class CharacterLevelTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "\n",
    "    def clean_sentence(self, sentence):\n",
    "        return str(sentence).lower()\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Monta o vocabulário a partir de uma lista de sentenças.\"\"\"\n",
    "        unique_chars = set()\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            cleaned_sentence = self.clean_sentence(sentence)\n",
    "            unique_chars.update(list(cleaned_sentence))\n",
    "\n",
    "        self.token_to_idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3} # Tokens especiais\n",
    "        for i, char in enumerate(sorted(list(unique_chars)), 4):\n",
    "            self.token_to_idx[char] = i\n",
    "\n",
    "        self.idx_to_token = {i: c for c, i in self.token_to_idx.items()}\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"Converte uma sentença em uma lista de caracteres (tokens).\"\"\"\n",
    "        cleaned_sentence = self.clean_sentence(sentence)\n",
    "        return list(cleaned_sentence)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converte uma lista de tokens (caracteres) em seus IDs correspondentes.\"\"\"\n",
    "        return [self.token_to_idx.get(token, self.token_to_idx['[UNK]']) for token in tokens]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "tokenizer = CharacterLevelTokenizer()\n",
    "tokenizer.build_vocab(sentences)\n",
    "print(f\"\\nTamanho do vocabulário: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "6906fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'h', 'e', ' ', 'h', 'a', 's', ' ', 'a', ' ', 't', 'e', 'r', 'r', 'i', 'b', 'l', 'e', ' ', 'd', 'i', 's', 'e', 'a', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"She has a terrible disease\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "bcf3e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class MostFrequentWordsTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "\n",
    "    def clean_sentence(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        return re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(self.clean_sentence(str(sentence)).split())\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        most_common_words = word_counts.most_common(self.vocab_size - 4)\n",
    "\n",
    "        self.token_to_idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3} # Tokens especiais\n",
    "        for i, (word, _) in enumerate(most_common_words, 4):\n",
    "            self.token_to_idx[word] = i\n",
    "\n",
    "        self.idx_to_token = {i: w for w, i in self.token_to_idx.items()}\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        cleaned_sentence = self.clean_sentence(str(sentence))\n",
    "        return cleaned_sentence.split()\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_idx.get(token, self.token_to_idx['[UNK]']) for token in tokens]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "tokenizer = MostFrequentWordsTokenizer(vocab_size=256)\n",
    "tokenizer.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "aaa0e083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'has', 'a', 'terrible', 'disease']"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"She has a terrible disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57d173",
   "metadata": {},
   "source": [
    "### Nível de sub-palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "9fb3a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class BytePairEncodingTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"] # Tokens especiais\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        trainer = BpeTrainer(vocab_size=self.vocab_size, special_tokens=self.special_tokens)\n",
    "        self.tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.tokenizer.token_to_id(token) for token in tokens]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "    \n",
    "tokenizer = BytePairEncodingTokenizer(vocab_size=2048)\n",
    "tokenizer.build_vocab(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "b153c9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'has', 'a', 'ter', 'r', 'ib', 'le', 'disease']"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"She has a terrible disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7ead8",
   "metadata": {},
   "source": [
    "## Conversão para IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "0e1e6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229, 218, 65, 157, 82, 274, 129, 590]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"She has a terrible disease\")\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e2db6",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "11304059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "VOCAB_SIZE = 256\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "tokenizer = MostFrequentWordsTokenizer(vocab_size=256)\n",
    "tokenizer.build_vocab(sentences)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "67638fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões dos embeddings: torch.Size([3, 768])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"She is sick\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "embeddings = embedding_layer(torch.tensor(ids))\n",
    "\n",
    "print(f'Dimensões dos embeddings: {embeddings.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "f430927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0037,  0.5122, -1.6622,  ..., -0.1847, -0.2666,  0.4592],\n",
       "        [-0.2956,  2.0093, -0.9796,  ..., -1.3825,  1.6764, -1.3256],\n",
       "        [-1.2131, -0.6097, -0.1351,  ..., -0.8753,  2.3371,  0.6921]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
