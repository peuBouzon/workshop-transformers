{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d486a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import re\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "232cc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration for Multimodal Model ---\n",
    "CONFIG = {\n",
    "    \"max_text_len\": 128,   # Max length for text tokens\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"d_model\": 128,        # Must be consistent for both modalities\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 2,\n",
    "    \"batch_size\": 16,      # Smaller batch size for larger model\n",
    "    \"epochs\": 25,\n",
    "    \"lr\": 1e-4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"vocab_size\": 5000,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2360e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTANT: Update this path to your folder containing the images ---\n",
    "IMAGE_DIR = '/home/pedrobouzon/life/datasets/pad-ufes-20/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b3efbbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Data Loading and Preprocessing ---\n",
    "print(\"Loading CSV data...\")\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45293665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          benign\n",
       "1       malignant\n",
       "2          benign\n",
       "3          benign\n",
       "4       malignant\n",
       "          ...    \n",
       "2293       benign\n",
       "2294    malignant\n",
       "2295       benign\n",
       "2296    malignant\n",
       "2297       benign\n",
       "Name: diagnostic, Length: 2298, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.loc[:, 'diagnostic'] = \n",
    "df['diagnostic'].map({\n",
    "  'BCC': 'malignant',\n",
    "  'SCC': 'malignant',\n",
    "  'ACK': 'benign',\n",
    "  'NEV': 'benign',\n",
    "  'SEK': 'benign',\n",
    "  'MEL': 'malignant'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87cb9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(df['diagnostic'].unique())\n",
    "label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "df['label'] = df['diagnostic'].map(label_to_int)\n",
    "NUM_CLASSES = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03d86097",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3e31b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Tokenizer and Dataset ---\n",
    "# (Using the same SimpleTokenizer from the BERT example)\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_idx = {}\n",
    "    def build_vocab(self, sentences):\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(str(sentence).lower().split())\n",
    "        word_counts = Counter(words)\n",
    "        most_common_words = word_counts.most_common(self.vocab_size - 4)\n",
    "        self.word_to_idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n",
    "        for i, (word, _) in enumerate(most_common_words, 4): self.word_to_idx[word] = i\n",
    "    def tokenize(self, sentence): return str(sentence).lower().split()\n",
    "    def convert_tokens_to_ids(self, tokens): return [self.word_to_idx.get(t, 1) for t in tokens]\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=CONFIG['vocab_size'])\n",
    "tokenizer.build_vocab(train_df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc6bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78fc0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ New MultiModal Dataset ✨\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, tokenizer, max_text_len, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "    def __len__(self): return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        # Text processing\n",
    "        text = row['sentence']\n",
    "        text_tokens = self.tokenizer.tokenize(text)\n",
    "        text_ids = self.tokenizer.convert_tokens_to_ids(text_tokens)[:self.max_text_len]\n",
    "        padding_len = self.max_text_len - len(text_ids)\n",
    "        text_ids = text_ids + [self.tokenizer.word_to_idx['[PAD]']] * padding_len\n",
    "        # Image processing\n",
    "        img_name = row['img_id']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        # Label\n",
    "        label = row['label']\n",
    "        return {\n",
    "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'image': image,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_dataset = MultiModalDataset(train_df, IMAGE_DIR, tokenizer, CONFIG['max_text_len'], transform)\n",
    "val_dataset = MultiModalDataset(val_df, IMAGE_DIR, tokenizer, CONFIG['max_text_len'], transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d7d467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Reusing and Building the Model ---\n",
    "# Reusing previously defined modules: SingleHeadAttention, MultiHeadAttention, FeedForward, TransformerEncoderBlock, PatchEmbedding\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, head_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.fc_q, self.fc_k, self.fc_v = [nn.Linear(d_model, head_dim) for _ in range(3)]\n",
    "        self.dropout, self.scale = nn.Dropout(dropout), torch.sqrt(torch.FloatTensor([head_dim])).to(CONFIG['device'])\n",
    "    def forward(self, x, mask=None):\n",
    "        Q, K, V = self.fc_q(x), self.fc_k(x), self.fc_v(x)\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None: energy = energy.masked_fill(mask.unsqueeze(1) == 0, -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        return torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(d_model, self.head_dim, dropout) for _ in range(n_heads)])\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        head_outputs = [head(x, mask) for head in self.heads]\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc_out(concatenated)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1, self.linear_2 = nn.Linear(d_model, ff_dim), nn.Linear(ff_dim, d_model)\n",
    "        self.dropout, self.relu = nn.Dropout(dropout), nn.ReLU()\n",
    "    def forward(self, x): return self.linear_2(self.dropout(self.relu(self.linear_1(x))))\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x): return self.projection(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "# ✨ New MultiModal Transformer ✨\n",
    "class MultiModalTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size, max_text_len, image_size, patch_size,\n",
    "                 d_model, n_layers, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        # Text components\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Image components\n",
    "        self.patch_embedding = PatchEmbedding(image_size, patch_size, 3, d_model)\n",
    "        # Shared components\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "        total_seq_len = max_text_len + num_patches + 1 # +1 for [CLS] token\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, total_seq_len, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        # Modality embedding to distinguish text vs. image\n",
    "        self.modality_embedding = nn.Embedding(2, d_model) # 0 for text, 1 for image\n",
    "        \n",
    "        self.transformer_encoder = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text_ids, image):\n",
    "        batch_size = image.shape[0]\n",
    "        \n",
    "        # 1. Process Text\n",
    "        text_embeds = self.text_embedding(text_ids) # [batch_size, text_len, d_model]\n",
    "        text_modality = torch.zeros_like(text_ids, dtype=torch.long).to(CONFIG['device'])\n",
    "        text_modality_embeds = self.modality_embedding(text_modality)\n",
    "        \n",
    "        # 2. Process Image\n",
    "        patch_embeds = self.patch_embedding(image) # [batch_size, num_patches, d_model]\n",
    "        image_modality = torch.ones(patch_embeds.shape[:2], dtype=torch.long).to(CONFIG['device'])\n",
    "        image_modality_embeds = self.modality_embedding(image_modality)\n",
    "        \n",
    "        # 3. Combine sequences\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Add modality embeddings before concatenation\n",
    "        text_embeds += text_modality_embeds\n",
    "        patch_embeds += image_modality_embeds\n",
    "        \n",
    "        x = torch.cat((cls_tokens, text_embeds, patch_embeds), dim=1)\n",
    "        # x shape: [batch_size, 1 + text_len + num_patches, d_model]\n",
    "        \n",
    "        # 4. Add positional embedding\n",
    "        x += self.position_embedding\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 5. Pass through Transformer Encoder\n",
    "        for layer in self.transformer_encoder:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # 6. Get [CLS] token output and classify\n",
    "        cls_output = x[:, 0]\n",
    "        return self.mlp_head(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2fc4cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting MultiModal Transformer training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 1.800 | Val. Loss: 1.688 | Val. Acc: 22.44%\n",
      "Epoch: 02 | Train Loss: 1.546 | Val. Loss: 1.408 | Val. Acc: 30.96%\n",
      "Epoch: 03 | Train Loss: 1.338 | Val. Loss: 1.230 | Val. Acc: 50.05%\n",
      "Epoch: 04 | Train Loss: 1.257 | Val. Loss: 1.107 | Val. Acc: 54.02%\n",
      "Epoch: 05 | Train Loss: 1.159 | Val. Loss: 1.041 | Val. Acc: 54.15%\n",
      "Epoch: 06 | Train Loss: 1.135 | Val. Loss: 1.063 | Val. Acc: 51.85%\n",
      "Epoch: 07 | Train Loss: 1.095 | Val. Loss: 1.046 | Val. Acc: 58.73%\n",
      "Epoch: 08 | Train Loss: 1.065 | Val. Loss: 0.973 | Val. Acc: 56.28%\n",
      "Epoch: 09 | Train Loss: 1.068 | Val. Loss: 0.987 | Val. Acc: 61.79%\n",
      "Epoch: 10 | Train Loss: 1.046 | Val. Loss: 0.978 | Val. Acc: 57.77%\n",
      "Epoch: 11 | Train Loss: 1.015 | Val. Loss: 0.986 | Val. Acc: 57.65%\n",
      "Epoch: 12 | Train Loss: 0.990 | Val. Loss: 0.945 | Val. Acc: 59.43%\n",
      "Epoch: 13 | Train Loss: 0.999 | Val. Loss: 0.962 | Val. Acc: 59.74%\n",
      "Epoch: 14 | Train Loss: 0.962 | Val. Loss: 0.960 | Val. Acc: 62.53%\n",
      "Epoch: 15 | Train Loss: 0.954 | Val. Loss: 0.935 | Val. Acc: 60.86%\n",
      "Epoch: 16 | Train Loss: 0.918 | Val. Loss: 0.947 | Val. Acc: 62.31%\n",
      "Epoch: 17 | Train Loss: 0.911 | Val. Loss: 0.935 | Val. Acc: 62.93%\n",
      "Epoch: 18 | Train Loss: 0.902 | Val. Loss: 0.939 | Val. Acc: 63.29%\n",
      "Epoch: 19 | Train Loss: 0.874 | Val. Loss: 0.926 | Val. Acc: 64.98%\n",
      "Epoch: 20 | Train Loss: 0.907 | Val. Loss: 0.940 | Val. Acc: 63.25%\n",
      "Epoch: 21 | Train Loss: 0.851 | Val. Loss: 0.956 | Val. Acc: 61.23%\n",
      "Epoch: 22 | Train Loss: 0.901 | Val. Loss: 0.950 | Val. Acc: 64.00%\n",
      "Epoch: 23 | Train Loss: 0.858 | Val. Loss: 0.928 | Val. Acc: 62.55%\n",
      "Epoch: 24 | Train Loss: 0.823 | Val. Loss: 0.917 | Val. Acc: 60.74%\n",
      "Epoch: 25 | Train Loss: 0.825 | Val. Loss: 0.918 | Val. Acc: 61.02%\n",
      "\n",
      "Final MultiModal Evaluation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.81      0.79      0.80       146\n",
      "         BCC       0.78      0.53      0.63       169\n",
      "         MEL       0.42      0.80      0.55        10\n",
      "         NEV       0.53      0.59      0.56        49\n",
      "         SCC       0.24      0.51      0.33        39\n",
      "         SEK       0.43      0.43      0.43        47\n",
      "\n",
      "    accuracy                           0.61       460\n",
      "   macro avg       0.54      0.61      0.55       460\n",
      "weighted avg       0.67      0.61      0.63       460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Training and Evaluation ---\n",
    "model = MultiModalTransformer(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    vocab_size=len(tokenizer.word_to_idx),\n",
    "    max_text_len=CONFIG['max_text_len'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    d_model=CONFIG['d_model'],\n",
    "    n_layers=CONFIG['n_layers'],\n",
    "    n_heads=CONFIG['n_heads'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "weights = 1 / (torch.bincount(torch.tensor(train_df['label'].values)) / len(train_df)).to(CONFIG['device'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        text_ids = batch['text_ids'].to(CONFIG['device'])\n",
    "        image = batch['image'].to(CONFIG['device'])\n",
    "        label = batch['label'].to(CONFIG['device'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text_ids, image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# ✨ New Evaluate Function ✨\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text_ids = batch['text_ids'].to(CONFIG['device'])\n",
    "            image = batch['image'].to(CONFIG['device'])\n",
    "            label = batch['label'].to(CONFIG['device'])\n",
    "            output = model(text_ids, image)\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss += loss.item()\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "    accuracy = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return epoch_loss / len(iterator), accuracy, all_preds, all_labels\n",
    "\n",
    "# ✨ Full Training and Evaluation Loop ✨\n",
    "print(\"\\nStarting MultiModal Transformer training...\")\n",
    "# The loop below will only work if you have the images and have set the IMAGE_DIR correctly.\n",
    "# It is commented out to prevent errors in this environment.\n",
    "min_loss = float('inf')\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy, _, _ = evaluate(model, val_loader, criterion)\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "    print(f\"Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {val_loss:.3f} | Val. Acc: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "state_dict = torch.load('best_multimodal_model.pth')\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "# ✨ Final Evaluation with Classification Report ✨\n",
    "print(\"\\nFinal MultiModal Evaluation...\")\n",
    "# Note: This will only produce meaningful results after the training loop has run.\n",
    "_, _, val_preds, val_labels = evaluate(model, val_loader, criterion)\n",
    "report = classification_report(\n",
    "    val_labels,\n",
    "    val_preds,\n",
    "    target_names=[int_to_label[i] for i in range(NUM_CLASSES)],\n",
    "    zero_division=0\n",
    ")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
