{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1d21936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, classification_report\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c33b6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration & Hyperparameters for ViT ---\n",
    "CONFIG = {\n",
    "    \"image_size\": 224,     # Standard ViT input size\n",
    "    \"patch_size\": 16,      # Standard ViT patch size\n",
    "    \"in_channels\": 3,      # RGB images\n",
    "    \"d_model\": 128,        # Embedding dimension\n",
    "    \"n_heads\": 4,          # Number of attention heads\n",
    "    \"n_layers\": 2,         # Number of Transformer blocks\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 25,\n",
    "    \"lr\": 1e-3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9a4ceabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✨ ==================================================================== ✨\n",
    "# ✨ IMPORTANT: Update this path to your folder containing the images     ✨\n",
    "IMAGE_DIR = '/home/pedrobouzon/life/datasets/pad-ufes-20/images/'\n",
    "# ✨ ==================================================================== ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9d9cadeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>diagnostic</th>\n",
       "      <th>diagnostic_number</th>\n",
       "      <th>img_id</th>\n",
       "      <th>folder</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PAT_1516</td>\n",
       "      <td>NEV</td>\n",
       "      <td>3</td>\n",
       "      <td>PAT_1516_1765_530.png</td>\n",
       "      <td>1</td>\n",
       "      <td>Patient History: Age: 8, Lesion region: arm, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAT_46</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>PAT_46_881_939.png</td>\n",
       "      <td>5</td>\n",
       "      <td>Patient History: Age: 55, Gender: female, Mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAT_1545</td>\n",
       "      <td>ACK</td>\n",
       "      <td>0</td>\n",
       "      <td>PAT_1545_1867_547.png</td>\n",
       "      <td>1</td>\n",
       "      <td>Patient History: Age: 77, Lesion region: face,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAT_1989</td>\n",
       "      <td>ACK</td>\n",
       "      <td>0</td>\n",
       "      <td>PAT_1989_4061_934.png</td>\n",
       "      <td>1</td>\n",
       "      <td>Patient History: Age: 75, Lesion region: hand,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAT_684</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>PAT_684_1302_588.png</td>\n",
       "      <td>1</td>\n",
       "      <td>Patient History: Age: 79, Gender: male, Matern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>PAT_1708</td>\n",
       "      <td>ACK</td>\n",
       "      <td>0</td>\n",
       "      <td>PAT_1708_3156_175.png</td>\n",
       "      <td>1</td>\n",
       "      <td>Patient History: Age: 73, Lesion region: hand,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>PAT_46</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>PAT_46_880_140.png</td>\n",
       "      <td>5</td>\n",
       "      <td>Patient History: Age: 55, Gender: female, Mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>PAT_1343</td>\n",
       "      <td>SEK</td>\n",
       "      <td>5</td>\n",
       "      <td>PAT_1343_1217_404.png</td>\n",
       "      <td>3</td>\n",
       "      <td>Patient History: Age: 74, Lesion region: forea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>PAT_326</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>PAT_326_690_823.png</td>\n",
       "      <td>5</td>\n",
       "      <td>Patient History: Age: 58, Gender: female, Mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>PAT_1714</td>\n",
       "      <td>SEK</td>\n",
       "      <td>5</td>\n",
       "      <td>PAT_1714_3189_989.png</td>\n",
       "      <td>5</td>\n",
       "      <td>Patient History: Age: 41, Lesion region: thigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2298 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id diagnostic  diagnostic_number                 img_id  folder  \\\n",
       "0      PAT_1516        NEV                  3  PAT_1516_1765_530.png       1   \n",
       "1        PAT_46        BCC                  1     PAT_46_881_939.png       5   \n",
       "2      PAT_1545        ACK                  0  PAT_1545_1867_547.png       1   \n",
       "3      PAT_1989        ACK                  0  PAT_1989_4061_934.png       1   \n",
       "4       PAT_684        BCC                  1   PAT_684_1302_588.png       1   \n",
       "...         ...        ...                ...                    ...     ...   \n",
       "2293   PAT_1708        ACK                  0  PAT_1708_3156_175.png       1   \n",
       "2294     PAT_46        BCC                  1     PAT_46_880_140.png       5   \n",
       "2295   PAT_1343        SEK                  5  PAT_1343_1217_404.png       3   \n",
       "2296    PAT_326        BCC                  1    PAT_326_690_823.png       5   \n",
       "2297   PAT_1714        SEK                  5  PAT_1714_3189_989.png       5   \n",
       "\n",
       "                                               sentence  \n",
       "0     Patient History: Age: 8, Lesion region: arm, L...  \n",
       "1     Patient History: Age: 55, Gender: female, Mate...  \n",
       "2     Patient History: Age: 77, Lesion region: face,...  \n",
       "3     Patient History: Age: 75, Lesion region: hand,...  \n",
       "4     Patient History: Age: 79, Gender: male, Matern...  \n",
       "...                                                 ...  \n",
       "2293  Patient History: Age: 73, Lesion region: hand,...  \n",
       "2294  Patient History: Age: 55, Gender: female, Mate...  \n",
       "2295  Patient History: Age: 74, Lesion region: forea...  \n",
       "2296  Patient History: Age: 58, Gender: female, Mate...  \n",
       "2297  Patient History: Age: 41, Lesion region: thigh...  \n",
       "\n",
       "[2298 rows x 6 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 2. Data Loading and Preprocessing ---\n",
    "print(\"Loading CSV data...\")\n",
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1f5e592e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEV', 'BCC', 'ACK', 'SEK', 'SCC', 'MEL'], dtype=object)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnostic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "61e98a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from diagnostic strings to integers\n",
    "labels = sorted(df['diagnostic'].unique())\n",
    "label_to_int = {label: i for i, label in enumerate(labels)} # -> (0, ACK), (1, BCC), ...\n",
    "int_to_label = {i: label for label, i in label_to_int.items()} # -> (key, value)\n",
    "df['label'] = df['diagnostic'].map(label_to_int) # ACK -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4d7babd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG['num_classes'] = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1557d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into training and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "096c41d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.367791\n",
       "0    0.317737\n",
       "3    0.106094\n",
       "5    0.102285\n",
       "4    0.083243\n",
       "2    0.022851\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts() / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1454faae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_percentage_train = train_df['label'].value_counts() / len(train_df)\n",
    "class_percentage_val = val_df['label'].value_counts() / len(val_df)\n",
    "\n",
    "np.allclose(class_percentage_train, class_percentage_val, atol=0.01) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a863b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6deccb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Custom PyTorch Dataset for CSV and Images ---\n",
    "class SkinLesionImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image ID and label from the dataframe\n",
    "        img_name = self.dataframe.iloc[idx]['img_id']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        # Construct the full image path\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        # Load the image using Pillow\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Define image transformations\n",
    "# We resize to the ViT's expected input size and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # -> [-1, 1]\n",
    "])\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = SkinLesionImageDataset(train_df, IMAGE_DIR, transform=transform)\n",
    "val_dataset = SkinLesionImageDataset(val_df, IMAGE_DIR, transform=transform)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9e91bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, head_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.fc_q = nn.Linear(d_model, head_dim)\n",
    "        self.fc_k = nn.Linear(d_model, head_dim)\n",
    "        self.fc_v = nn.Linear(d_model, head_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([head_dim])).to(CONFIG['device'])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q, K, V = self.fc_q(x), self.fc_k(x), self.fc_v(x)\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask.unsqueeze(1) == 0, -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        return torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            SingleHeadAttention(d_model, self.head_dim, dropout)\n",
    "            for _ in range(n_heads)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        head_outputs = [head(x, mask) for head in self.heads]\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc_out(concatenated)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, ff_dim)\n",
    "        self.linear_2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(self.relu(self.linear_1(x))))\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "class PatchEmbeddingConv(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, d_model, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts an image into a sequence of flattened patch embeddings using einops.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # The size of a single flattened patch vector\n",
    "        patch_vector_dim = patch_size * patch_size * in_channels\n",
    "        \n",
    "        # A standard Linear layer to project the flattened patch vector\n",
    "        self.projection = nn.Linear(patch_vector_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, in_channels, height, width]\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', \n",
    "                      p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        return self.projection(x)\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, num_classes,\n",
    "                 d_model, n_layers, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, d_model)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        self.transformer_encoder = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embedding\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.transformer_encoder:\n",
    "            x = layer(x)\n",
    "        cls_output = x[:, 0]\n",
    "        return self.mlp_head(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3712cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights \n",
      "ACK: 3.15\n",
      "BCC: 2.72\n",
      "MEL: 43.76\n",
      "NEV: 9.43\n",
      "SCC: 12.01\n",
      "SEK: 9.78\n"
     ]
    }
   ],
   "source": [
    "weights = 1 / (torch.bincount(torch.tensor(train_df['label'].values)) / len(train_df))\n",
    "\n",
    "print('Class weights ')\n",
    "for i, label in int_to_label.items():\n",
    "  print(f'{label}: {weights[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "47b4e137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting ViT training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 1.907 | Val. Loss: 1.854 | Val. Recall: 16.67%\n",
      "Epoch: 02 | Train Loss: 1.829 | Val. Loss: 1.786 | Val. Recall: 20.77%\n",
      "Epoch: 03 | Train Loss: 1.781 | Val. Loss: 1.718 | Val. Recall: 24.78%\n",
      "Epoch: 04 | Train Loss: 1.683 | Val. Loss: 1.615 | Val. Recall: 30.40%\n",
      "Epoch: 05 | Train Loss: 1.617 | Val. Loss: 1.469 | Val. Recall: 35.03%\n",
      "Epoch: 06 | Train Loss: 1.512 | Val. Loss: 1.381 | Val. Recall: 39.83%\n",
      "Epoch: 07 | Train Loss: 1.513 | Val. Loss: 1.362 | Val. Recall: 41.52%\n",
      "Epoch: 08 | Train Loss: 1.365 | Val. Loss: 1.315 | Val. Recall: 51.72%\n",
      "Epoch: 09 | Train Loss: 1.360 | Val. Loss: 1.302 | Val. Recall: 45.61%\n",
      "Epoch: 10 | Train Loss: 1.313 | Val. Loss: 1.279 | Val. Recall: 51.44%\n",
      "Epoch: 11 | Train Loss: 1.268 | Val. Loss: 1.237 | Val. Recall: 51.27%\n",
      "Epoch: 12 | Train Loss: 1.272 | Val. Loss: 1.286 | Val. Recall: 44.95%\n",
      "Epoch: 13 | Train Loss: 1.266 | Val. Loss: 1.305 | Val. Recall: 47.01%\n",
      "Epoch: 14 | Train Loss: 1.271 | Val. Loss: 1.371 | Val. Recall: 46.42%\n",
      "Epoch: 15 | Train Loss: 1.303 | Val. Loss: 1.293 | Val. Recall: 44.89%\n",
      "Epoch: 16 | Train Loss: 1.250 | Val. Loss: 1.229 | Val. Recall: 49.24%\n",
      "Epoch: 17 | Train Loss: 1.192 | Val. Loss: 1.235 | Val. Recall: 51.64%\n",
      "Epoch: 18 | Train Loss: 1.194 | Val. Loss: 1.307 | Val. Recall: 50.00%\n",
      "Epoch: 19 | Train Loss: 1.152 | Val. Loss: 1.299 | Val. Recall: 48.89%\n",
      "Epoch: 20 | Train Loss: 1.143 | Val. Loss: 1.219 | Val. Recall: 50.95%\n",
      "Epoch: 21 | Train Loss: 1.068 | Val. Loss: 1.415 | Val. Recall: 43.96%\n",
      "Epoch: 22 | Train Loss: 1.129 | Val. Loss: 1.261 | Val. Recall: 51.45%\n",
      "Epoch: 23 | Train Loss: 1.120 | Val. Loss: 1.318 | Val. Recall: 48.00%\n",
      "Epoch: 24 | Train Loss: 1.080 | Val. Loss: 1.239 | Val. Recall: 51.08%\n",
      "Epoch: 25 | Train Loss: 1.085 | Val. Loss: 1.256 | Val. Recall: 46.50%\n",
      "\n",
      "Final ViT Evaluation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.64      0.63      0.63       146\n",
      "         BCC       0.74      0.41      0.53       169\n",
      "         MEL       0.28      0.50      0.36        10\n",
      "         NEV       0.44      0.49      0.46        49\n",
      "         SCC       0.20      0.38      0.26        39\n",
      "         SEK       0.41      0.64      0.50        47\n",
      "\n",
      "    accuracy                           0.51       460\n",
      "   macro avg       0.45      0.51      0.46       460\n",
      "weighted avg       0.59      0.51      0.53       460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Training and Evaluation Loop ---\n",
    "model = VisionTransformer(\n",
    "    image_size=CONFIG['image_size'],\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    in_channels=CONFIG['in_channels'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    d_model=CONFIG['d_model'],\n",
    "    n_layers=CONFIG['n_layers'],\n",
    "    n_heads=CONFIG['n_heads'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "weights = 1 / (torch.bincount(torch.tensor(train_df['label'].values)) / len(train_df)).to(CONFIG['device'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, labels in iterator:\n",
    "        images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in iterator:\n",
    "            images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    accuracy = recall_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss / len(iterator), accuracy, all_preds, all_labels\n",
    "\n",
    "print(\"\\nStarting ViT training...\")\n",
    "# The loop below will only work if you have the images and have set the IMAGE_DIR correctly.\n",
    "# It is commented out to prevent errors in this environment.\n",
    "min_loss = float('inf')\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy, _, _ = evaluate(model, val_loader, criterion)\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "    print(f\"Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {val_loss:.3f} | Val. Recall: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "state_dict = torch.load('best_vit_model.pth')\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "print(\"\\nFinal ViT Evaluation...\")\n",
    "# Note: This will only produce meaningful results after the training loop has run.\n",
    "_, _, val_preds, val_labels = evaluate(model, val_loader, criterion)\n",
    "report = classification_report(\n",
    "    val_labels,\n",
    "    val_preds,\n",
    "    target_names=[int_to_label[i] for i in range(CONFIG['num_classes'])],\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
